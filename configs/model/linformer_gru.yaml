_target_: src.models.ch_lit_module.CHLitModule
_recursive_: True

conditional_metric: ${conditional_metric}

train_batch_size: ${data.train_batch_size}
val_batch_size: ${data.val_batch_size}

task_names: 
  - base_output

task_weights:
  - 1.0

metric_names: 
  - gini

optimizer: 
  _target_: torch.optim.Adam
  _partial_: True
  lr: 1.0e-3
  weight_decay: 1.0e-5

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: True
  mode: max
  factor: 0.1 # new_lr = factor & old_lr
  patience: 3

net:
  _target_: src.models.components.sequential_model.SequentialLitModel
  embedding_dim: 16

  layers:
    ### Encoder layer (Embeddings)
    - _target_: src.models.components.embedding.encoder_layer.EncoderLayer

      numerical_features: ${data.features.numerical}
      categorical_features: ${data.features.categorical}

      embedding_dim: ${model.net.embedding_dim}

      dropout_inputs: 0.3

      non_linear: False
      num_batch_norm: True

    
    ### Base Transformer
    - _target_: src.models.components.base_transformer.transformer.BaseTransformer

      use_decoder: False

      ### Encoder
      encoder:
        _target_: src.models.components.base_transformer.encoder_decoder.Encoder

        n_layers: 1

        encoder_layer: 
          _target_: src.models.components.base_transformer.sub_layers.EncoderLayer

          attention: 
            _target_: src.models.components.base_transformer.multihead_attention.MultiHeadAttention

            d_model: ${model.net.embedding_dim}

            seq_len: ${data.collator.max_seq_len}

            d_k: ${eval:${model.net.embedding_dim} // 4}

            n_heads: 4

            attn_dropout: 0.1

          feed_forward:
            _target_: src.models.components.base_transformer.positionwise_ff.PositionwiseFeedForward

            d_model: ${model.net.embedding_dim}
            d_ff: ${eval:${model.net.embedding_dim} // 2}

            activation_type: relu

          residual_dropout: 0.3


    ### Decoder layer (GRU)
    - _target_: src.models.components.aggregators.gru_aggregator.GRUAggregator

      hidden_size: ${model.net.embedding_dim}

      num_layers_gru: 1
      bidirectional: False

      dropout_gru: 0.0


    ### Use main sequence only
    - _target_: src.models.components.seq_to_seq.main_seq.UseMainSeq


    ### Multioutput Linear Block
    - _target_: src.models.components.linear_blocks.linear_conv_blocks.MultiTaskLinearBlock
      heads:
        - _target_: src.models.components.linear_blocks.linear_conv_blocks.LinearBlock
          in_features: ${model.net.embedding_dim}
          out_features: 1
          num_layers: 2
          activation_type: tanh

compile: False
