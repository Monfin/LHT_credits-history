# @package _global_

# to execute this experiment run:
# python train.py +experiment=example

# defaults:
#   - override /data: credits_history_dataset.yaml
#   - override /model: embeddings_gru_pooling.yaml
#   - override /trainer: cpu.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["credits_history", "linformer"]

seed: 12345

logger:
  mlflow:
    experiment_name: linformer_emb16
    run_name: credits_history

# set data params
data:
  data_reader:
    _target_: src.data.components.data_reader.DataReader
    data_path: ${paths.data_dir}/serialized/serialized_0_v5

  targets_reader: 
    _target_: src.data.components.targets_indexes_reader.TargetsReader
    targets_path: ${paths.data_dir}/targets/targets_dict.pickle

  indexes_reader: 
    _target_: src.data.components.targets_indexes_reader.IndexesReader
    train_path: ${paths.data_dir}/indexes/ser_full_0_indexes/train_indexes.pickle
    val_path: ${paths.data_dir}/indexes/ser_full_0_indexes/valid_indexes.pickle
    test_path: null

  collator:
    _target_: src.data.components.collate.BaseCollator
    max_seq_len: 20

  balance_sampler: balanced

  train_batch_size: 32
  val_batch_size: 32


effective_batch_size: 256


# set trainer params
trainer:
  accumulate_grad_batches: ${eval:${effective_batch_size} // ${data.train_batch_size}}

  min_epochs: 1 # prevents early stopping
  max_epochs: 20

  log_every_n_steps: 30


# set model params
model:
  _target_: src.models.ch_lit_module.CHLitModule
  _recursive_: True

  conditional_metric: ${conditional_metric}

  train_batch_size: ${data.train_batch_size}
  val_batch_size: ${data.val_batch_size}

  task_names: 
    - base_output

  task_weights:
    - 1.0

  metric_names: 
    - gini

  optimizer: 
    _target_: torch.optim.Adam
    _partial_: True
    lr: 1.0e-3
    weight_decay: 1.0e-5

  scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    _partial_: True
    mode: max
    factor: 0.1 # new_lr = factor & old_lr
    patience: 2

  net:
    _target_: src.models.components.sequential_model.SequentialLitModel
    embedding_dim: 16

    layers:
      ### Encoder layer (Embeddings)
      - _target_: src.models.components.embedding.encoder_layer.EncoderLayer

        numerical_features: ${data.features.numerical}
        categorical_features: ${data.features.categorical}

        embedding_dim: ${model.net.embedding_dim}

        dropout_inputs: 0.3

        non_linear: False
        num_batch_norm: False

      
      ### Base Transformer
      - _target_: src.models.components.base_transformer.transformer.BaseTransformer

        ### Encoder
        encoder:
          _target_: src.models.components.base_transformer.encoder_decoder.Encoder

          n_layers: 1

          encoder_layer: 
            _target_: src.models.components.base_transformer.sub_layers.EncoderLayer

            attention: 
              _target_: src.models.components.base_transformer.multihead_attention.MultiHeadAttention

              d_model: ${model.net.embedding_dim}

              seq_len: ${data.collator.max_seq_len}

              d_proj: ${eval:${model.net.embedding_dim} // 2}

              n_heads: 4

              dropout: 0.1

            feed_forward:
              _target_: src.models.components.base_transformer.positionwise_ff.PositionwiseFeedForward

              d_model: ${model.net.embedding_dim}
              d_ff: ${eval:${model.net.embedding_dim} // 2}

              activation_type: relu

            residual_dropout: 0.1


      ### Branched Decoder layer (GRU + ConvPooling)
      - _target_: src.models.components.aggregators.branched_aggregator.BranchedAggregator
        embedding_dim: ${model.net.embedding_dim}

        branches:
          ### GRU layer (GRU)
          - _target_: src.models.components.aggregators.gru_aggregator.GRUAggregator

            hidden_size: ${model.net.embedding_dim}

            num_layers_gru: 1
            bidirectional: False

            dropout_gru: 0.0

          ### Pooling layer (Convolutional & Pooling)
          - _target_: src.models.components.pooling.agg_pooling.ConvPooling
            emb_dim: ${model.net.embedding_dim}
            pooling_type: all
            use_batch_norm: False


      ### Use main sequence only
      - _target_: src.models.components.seq_to_seq.states_transformation.SFS2ModelOutput


      ### Multioutput Linear Block
      - _target_: src.models.components.linear_blocks.linear_conv_blocks.MultiTaskLinearBlock
        heads:
          - _target_: src.models.components.linear_blocks.linear_conv_blocks.LinearBlock
            in_features: ${model.net.embedding_dim}
            out_features: 1
            num_layers: 1
            activation_type: tanh